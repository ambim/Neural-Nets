{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN06 - Gibbs sampling, RBM's, MCMC, generative models\n",
    "\n",
    "In this notebook: \n",
    "- gibbs sampling\n",
    "- boltzmann machines\n",
    "- RBMs\n",
    "- generative models\n",
    "- contrastive divergence\n",
    "\n",
    "\n",
    "\n",
    "## Boltzmann machines\n",
    "A Boltzmann machine: \n",
    "- is a stochastic variant of the Hopfield network. \n",
    "- it uses the Boltzmann distribution as a sampling function. \n",
    "- good for learning joint data distributions. \n",
    "- slow in practice, but efficient with restricted connectivity. \n",
    "\n",
    "Now neurons are *on* (resp. *off*) __with probability__ *p<sub>i</sub>*. The energy E<sub>i</sub> of a Boltzmann machine is similar to that of a Hopfield net. The use of bias (denoted W<sub>0i</sub> or θ<sub>i</sub>): \n",
    "<img src=\"energy2.png\",width=400,height=400>\n",
    "\n",
    "The network learns a joint distribution on the visible units. \n",
    "<img src=\"bm.png\",width=200,height=200>\n",
    "The difference in energy when single unit *i* is flipped from *off* (zero) to *on* (one) is given by: \n",
    "<img src=\"energy3.png\",width=400,height=400>\n",
    "\n",
    "#### Boltzmann learning rule\n",
    "- **Positive phase:** visibile units' states are clamped to a particular binary state vector sampled from the training set \n",
    "- **Negative phase:** the network is allowed to run freely, i.e. no units have their state determined by external data. \n",
    "<img src=\"learnings.png\",width=200,height=200>\n",
    "- *p+* is probability of units *i* and *j* both being on when the machine is at equilibrium on the positive phase. \n",
    "- *p-* is probability of units *i* and *j* both being on when the machine is at equilibrium on the negative phase.\n",
    "\n",
    "#### Gibbs sampling\n",
    "1. Start with an arbitrary state\n",
    "2. Neurons are repeatedly visited in order\n",
    "3. Calculate new value for neuron i in {0,1} according to: \n",
    "\n",
    "p<sub>i</sub> = 1/ (1 +exp(-∆E<sub>i</sub>/T)) ...until the network reaches thermal equilibirum. \n",
    "\n",
    "#### MCMC (Markov chan Monte Carlo) \n",
    "Markov chain monte carlo methods (like Gibbs sampling provide a way of approximating the value of an integral...\n",
    "\n",
    "**Monte Carlo** = random sampling i.e. from a uniform distribution\n",
    "\n",
    "**Markov chain** = walking the right way (sampling from a distribution P(x) that is not uniform); i.e. make the likelihood of visiting a point *x* proportional to P(x) so that x<sup>t+1</sup> depends only on x<sup>t</sup> \n",
    "\n",
    "#### Combining probability models\n",
    "- **Mixture**: Take a weighted average of the distributions (never to be sharper than the individual distributions). \n",
    "- **Product**: multiply the distributions at each point and then re-normalize (this is how an RB combines the distributions defined by the hidden units). More powerful than a mixture, but the normalization makes maximum likelihood learning difficult; but approximations allow learning. \n",
    "- **Composition** - use the values of the latent variables of one model as the data for the next model. Works well for learning multiple layers of representation, but only if the individual models are undirected (i.e. symmetrical networks). \n",
    "\n",
    "#### Generative neural networks\n",
    "If we connect binary stochastic neurons in a directed acyclic graph we get a **Sigmoid Belief Net** (Neal, 1992). \n",
    "\n",
    "If we connect binary stochastic neurons using symmetric connections we get a **Boltzmann machine** (Hinton and Sejnowski, 1983). If we restrict the connectivity in a special way we get a **Restricted Boltzmann Machine**, which is easy to learn. \n",
    "    \n",
    "## Restricted Boltzmann Machine (RBM) \n",
    "(*...combining all the above elements*) \n",
    "<img src=\"rbm.png\",width=200,height=200>\n",
    "Restrict the connectivity of the Boltzmann machine to make learning easier: \n",
    "- only one layer of hidden units (apart from a deep BM, see below). \n",
    "- no connections between hidden units or between visible units. \n",
    "\n",
    "Thus, in an RBM, the hidden units are conditionally independent given the visible states. \n",
    "\n",
    "**Energy of an RBM**: \n",
    "<img src=\"energy4.png\",width=400,height=400>\n",
    "\n",
    "**RBM learning**: \n",
    "<img src=\"rbmlearn.png\",width=400,height=400>\n",
    "\n",
    "**Contrastive divergence (CD1) method of learning an RBM**: \n",
    "<img src=\"cd1.png\",width=400,height=400>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
